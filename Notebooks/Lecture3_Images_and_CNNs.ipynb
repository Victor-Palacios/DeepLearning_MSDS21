{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e032868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb290b-54b8-4d2f-af18-fff71dcd33ab",
   "metadata": {},
   "source": [
    "Image Data from [here](https://www.kaggle.com/andrewmvd/animal-faces)\n",
    "- Animal Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea405e99-a951-4762-af0d-93a1f7b844b6",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b39fed-5b4f-4bec-96b3-b1bca2670727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's in this dataset?\n",
    "import os\n",
    "os.listdir('course_data/afhq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a10db-0c51-4d07-8fa1-7c198b919450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three labels\n",
    "os.listdir('course_data/afhq/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39b04d-74c5-4fb3-9a25-9204b2f8afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within each folder are the images\n",
    "os.listdir('course_data/afhq/train/cat')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d8a22-1b81-4ac9-b113-55e1fc471fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for our data\n",
    "data_path = 'course_data/afhq'\n",
    "\n",
    "rows = []\n",
    "for dataset in os.listdir(data_path):\n",
    "    for label in os.listdir(data_path + f'/{dataset}'):\n",
    "        for image in os.listdir(data_path + f'/{dataset}' + f'/{label}'):\n",
    "            row = dict()\n",
    "            row['image_file'] = image\n",
    "            row['label'] = label\n",
    "            row['dataset'] = dataset\n",
    "        \n",
    "            # a bit redudant, could build from other data in __getitem__ if wanted\n",
    "            row['image_path'] = data_path + f'/{dataset}' + f'/{label}'\n",
    "            rows.append(row)\n",
    "        \n",
    "df = pd.DataFrame(rows)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a6797-0867-4da1-abd4-7a547982d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation data\n",
    "df_train = df[df['dataset'] == 'train'].reset_index(drop=True)\n",
    "df_val = df[df['dataset'] == 'val'].reset_index(drop=True)\n",
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81c9e0-5cf7-4931-a34d-cef8f088c8b1",
   "metadata": {},
   "source": [
    "Before creating a Dataset class, let's think about what we want as our input to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3439e30c-3e2a-4f40-ab62-cc8b049f878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# pull up an image\n",
    "row = df.iloc[0]\n",
    "image_path = row['image_path']\n",
    "fname = row['image_file']\n",
    "path = image_path+'/'+fname\n",
    "img = cv2.imread(path)\n",
    "\n",
    "# what is an image?\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f057c43-0eff-4754-b45a-8ee432db0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 512x512 image with 3 channels\n",
    "print(img.shape)\n",
    "\n",
    "# pixel intensity goes from 0 to 255\n",
    "print(np.max(img), np.min(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad211e21-044b-44f2-a28d-12b1abe5adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the image\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe556d9b-77e8-4a54-9311-2526867a0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why is it weird? cv2 opens in BGR instead of RGB\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484f820-2061-4662-8c08-1bd52f6cfa77",
   "metadata": {},
   "source": [
    "## Convolutional Layers\n",
    "- [Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) for ```Conv2d``` is a must-read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0620919-793e-45c8-8e74-568fb0b7bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of channels of the input\n",
    "in_channels = 3\n",
    "# number of filters (hence number of output channels)\n",
    "out_channels = 32\n",
    "# filter size\n",
    "kernel_size = 3 # equivalent to (3,3)\n",
    "\n",
    "# define the layer\n",
    "conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "\n",
    "# why error? (two reasons!)\n",
    "conv(torch.tensor(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accca5f-14c5-40ed-9648-ff57c57e05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try again\n",
    "img2 = img[np.newaxis, :, :, :]\n",
    "img2 = np.transpose(img2, (0, 3, 1, 2))\n",
    "img2 = torch.tensor(img2).float()\n",
    "\n",
    "output = conv(img2)\n",
    "\n",
    "# why this shape?\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78240b9-0f24-44b0-8385-ca7dd810577a",
   "metadata": {},
   "source": [
    "Think: How can we change this so that the output has the same 2D shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d4787-85b5-4f59-943a-1e09ecc93f2d",
   "metadata": {},
   "source": [
    "## Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777d74d-9040-4dda-917c-63834ddc617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a Dataset for our animal faces! \n",
    "class AnimalFacesDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        # label dictionary\n",
    "        self.label_dict = {'cat':0, 'dog':1, 'wild':2}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # get ingredients for retrieving image\n",
    "        image_path = row['image_path']\n",
    "        fname = row['image_file']\n",
    "        path = image_path+'/'+fname\n",
    "        \n",
    "        # read the img\n",
    "        img = cv2.imread(path)\n",
    "        \n",
    "        # convert to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # move color channels to correct spot\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        \n",
    "        # convert to [0,1] scale\n",
    "        img = torch.tensor(img / 255.).float()\n",
    "        \n",
    "        label = torch.tensor(self.label_dict[row['label']])\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa53271-5aa5-40df-8a0e-d90b74dcb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = AnimalFacesDataset(df_train)\n",
    "dl_train = DataLoader(ds_train, batch_size = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c6612-15fa-463e-98ce-8dd856f040cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure our recipe works!\n",
    "# notice the time...\n",
    "for img, label in tqdm(dl_train):\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d8ac0-360c-4eca-81e7-bca2abe811c8",
   "metadata": {},
   "source": [
    "Have to sketch out dimensions while constructing!\n",
    "\n",
    "Input: (3, 512, 512)\n",
    "\n",
    "Conv1 -> (32, 512, 512)\n",
    "\n",
    "Pool -> (32, 256, 256)\n",
    "\n",
    "Conv2 -> (64, 256, 256)\n",
    "\n",
    "Pool -> (64, 128, 128)\n",
    "\n",
    "Conv3 -> (128, 128, 128)\n",
    "\n",
    "Pool -> (128, 64, 64)\n",
    "\n",
    "Conv4 -> (1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31eb6b-86f2-4cfc-a578-04b56b65de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # same padding!\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # doing this to shrink size enough!\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.linear1 = nn.Linear(4096, 100)\n",
    "        \n",
    "        # read documentation for CrossEntropy Loss!\n",
    "        self.linear2 = nn.Linear(100, 3)\n",
    "        \n",
    "        # pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # for unrolling into FC layer\n",
    "        self.unroll = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # helpful to do this along the way sometimes!\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # unroll x for FC layer\n",
    "        x = self.linear1(self.unroll(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e0e49-efd7-42cc-83ff-7e3115651629",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(img2)\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, input_size = (3, 512, 512), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac55bb-a11e-4877-9410-3a09d3e06c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
