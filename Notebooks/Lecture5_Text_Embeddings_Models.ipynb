{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adfd5c-c155-444d-beeb-89161a51b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0668f27-fd1f-4930-819f-fcbdb2702a9c",
   "metadata": {},
   "source": [
    "IMDB Movie Review Dataset (cleaned)\n",
    "- Originally from [here](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "- Cleaned into a csv [here](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14c97e-da75-46c3-aadb-e1576bedcf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('course_data/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73f20c-ac25-407e-8318-77680e570778",
   "metadata": {},
   "source": [
    "## Automatic Tokenization with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e50c35-b1b7-429d-8004-fbc1fdcb9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool for text\n",
    "import spacy\n",
    "\n",
    "# load information about words\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754d50b-148e-48a6-a470-0eac8236796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = df.iloc[9]['review']\n",
    "print(some_text)\n",
    "\n",
    "# automatically tokenize the text\n",
    "tokenized_text = nlp(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c905d-66ad-41b7-8ad5-ad0a85e41615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's not perfect\n",
    "for token in tokenized_text:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fae344-cb8d-411b-8c6a-ef3e226d7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "\n",
    "token = tokenized_text[idx]\n",
    "\n",
    "# lemmatization\n",
    "print('Lemmatization of', token.text, 'is', token.lemma_)\n",
    "\n",
    "# part of speech tagging\n",
    "print(token.text, 'is a', token.pos_)\n",
    "\n",
    "# is it a stop word?\n",
    "print('The fact that', token.text, 'is a stop word is', token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a84263-6ea6-4ebd-b092-9c69aa053e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence segmentation\n",
    "for sentence in tokenized_text.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb184749-fc09-425e-bd27-4f56759b4c0e",
   "metadata": {},
   "source": [
    "- tons more fancy features!\n",
    "- Let's do a simple pipeline where we ignore non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c291843-acb8-406c-bced-b8a477bfbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "a_review = df.iloc[9]['review']\n",
    "\n",
    "# remove those <br />s\n",
    "a_review = a_review.replace('<br />', ' ')\n",
    "print(a_review)\n",
    "\n",
    "# remove non-alphabetic characters\n",
    "a_review = re.sub(\"[^A-Za-z']+\", ' ', a_review)\n",
    "print(a_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a2ccd-516e-43ac-a485-8f03ca2032ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabling some fancy features of spacy for speed\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['ner', 'parser'])\n",
    "\n",
    "rows = []\n",
    "for idx in tqdm(range(len(df))):\n",
    "    row = df.iloc[idx].copy()\n",
    "    \n",
    "    # first we remove numeric characters and lowercase everything\n",
    "    cleaned_review = re.sub(\"[^A-Za-z']+\", ' ', row['review'].replace('<br />', ' ')).lower()\n",
    "    \n",
    "    # we let spaCy tokenize and lemmatize the text for us\n",
    "    tokenized_review = nlp(cleaned_review)\n",
    "    cleaned_tokenized = [token.lemma_ for token in tokenized_review if ((not token.is_stop) or (' ' in token.text))]\n",
    "    \n",
    "    if len(cleaned_tokenized) > 1:\n",
    "        row['cleaned'] = ' '.join(cleaned_tokenized)\n",
    "    rows.append(row)\n",
    "df_clean = pd.DataFrame(rows)\n",
    "df_clean.head()\n",
    "df_clean.to_csv('course_data/IMDB_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400239e5-9d76-462b-be23-c6165b7e2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('course_data/IMDB_cleaned.csv')\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd434d-a060-40b6-89b2-a7d745efe03c",
   "metadata": {},
   "source": [
    "## Prepare for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b07a32-67ad-4952-9acf-8fbf925d6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words, send infrequent to unknown\n",
    "\n",
    "# let's get an idea of word frequency\n",
    "from collections import Counter\n",
    "\n",
    "reviews = [review.split(' ') for review in list(df_clean['cleaned'])]\n",
    "word_freq = Counter([token for review in reviews for token in review]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d58387-6570-40aa-8190-8fa84fa28c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no surprises here\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73b89b-60b9-4600-bd31-8e8ab6fdba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words only seen once\n",
    "word_freq[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6d11b-bfeb-499d-bdb5-3f6bd9c62018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that appear infrequently\n",
    "word_freq = dict(word_freq)\n",
    "print(len(word_freq))\n",
    "min_freq = 5\n",
    "word_dict = {}\n",
    "\n",
    "# sending all the unknowns to 0\n",
    "i = 1\n",
    "for word in word_freq:\n",
    "    if word_freq[word] > min_freq:\n",
    "        word_dict[word] = i\n",
    "        i += 1\n",
    "    else:\n",
    "        word_dict[word] = 0\n",
    "\n",
    "# dictionary length        \n",
    "dict_length = max(word_dict.values()) + 1\n",
    "dict_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481996b9-1b4d-457f-869e-3665b48f0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to collate the tensors into batches, sentence need to be the same size\n",
    "# we could overwrite the collate function, or we could pick a max sentence size and pad\n",
    "\n",
    "max_length = 0\n",
    "for idx in tqdm(range(len(df_clean))):\n",
    "    row = df_clean.iloc[idx]\n",
    "    length = len(row['cleaned'].split(' '))\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d0945-6010-4589-9777-5777630b020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, df, word_dict, max_length):\n",
    "        self.df = df\n",
    "        self.word_dict = word_dict\n",
    "        self.sent_dict = {'negative': 0, 'positive': 1}\n",
    "        self.max_len = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        review = row['cleaned'].split(' ')\n",
    "        x = torch.zeros(self.max_len)\n",
    "        \n",
    "        # get review as a list of integers\n",
    "        for idx in range(len(review)):\n",
    "            # we want to front pad for RNN\n",
    "            x[self.max_len - len(review) + idx] = self.word_dict[review[idx]]\n",
    "            \n",
    "        y = torch.tensor(self.sent_dict[row['sentiment']]).float()\n",
    "        \n",
    "        # embedding likes long tensors\n",
    "        return x.long(), y\n",
    "ds = IMDBDataset(df_clean, word_dict, max_length)\n",
    "next(iter(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f05b5f-1c74-434c-bc72-b6a0f60dec48",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d591e-2aea-4faf-8c8f-6c4941524464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW model for sentiment analysis\n",
    "# train the embedding during training\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, dict_length, embedding_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        # padding index turns off gradient for unknown tokens\n",
    "        self.word_emb = nn.Embedding(dict_length, embedding_size, padding_idx=0)\n",
    "        self.linear = nn.Linear(embedding_size, 1)\n",
    "        self.emb_size = embedding_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sent_length = x.shape[1]\n",
    "        x = self.word_emb(x)\n",
    "        sent_length = torch.count_nonzero(x, dim=1)\n",
    "        x = torch.sum(x, dim=1) / sent_length\n",
    "        x = self.linear(x)\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d8654-d202-4297-88f8-025cb96d4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=1000, shuffle=True)\n",
    "x, y = next(iter(dl))\n",
    "\n",
    "cbow_model = CBOW(dict_length, 100)\n",
    "cbow_model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f2893-55e8-402c-ad06-a43ab14219e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_pass(model, dataloader, optimizer, lossFun, backwards=True, print_loss=False):\n",
    "    \n",
    "    if backwards == True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = lossFun(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if backwards == True:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if print_loss == True:\n",
    "        print(avg_loss)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def one_pass_acc(model, dataloader, num_points):\n",
    "    model.eval()\n",
    "    total_incorrect = 0\n",
    "        \n",
    "    for x, y in dataloader:\n",
    "        y_pred = (torch.sigmoid(model(x)) > 0.5).float()\n",
    "        \n",
    "        total_incorrect += torch.count_nonzero(y - y_pred).item()\n",
    "        \n",
    "    percent_wrong = total_incorrect / num_points\n",
    "    return 1 - percent_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d73c4a-61a4-47c3-8d65-8f319f4f572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFun = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr = 0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print('Epoch: ', epoch)\n",
    "    \n",
    "    loss = one_pass(cbow_model, dl, optimizer, lossFun)\n",
    "    print('Loss: ', loss)\n",
    "    \n",
    "    acc = one_pass_acc(cbow_model, dl, len(ds))\n",
    "    print('Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c25a5-3aa6-4325-b66e-2b1fc80daa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model for sentiment analysis (read Documentation for nn.RNN!)\n",
    "# train the embedding during training\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, dict_length, embedding_size):\n",
    "        super(RNN, self).__init__()\n",
    "        # padding index turns off gradient for unknown tokens\n",
    "        self.word_emb = nn.Embedding(dict_length, embedding_size, padding_idx=0)\n",
    "        \n",
    "        # RNN doesn't care about length of sequence\n",
    "        # RNN does care about the size of the word embedding\n",
    "        # hidden size dictates dimension of output of RNN\n",
    "        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=1, batch_first=True)\n",
    "        \n",
    "        # PyTorch RNN outputs a sequence of same length as input\n",
    "        # For many to one, we can either use the final hidden state OR\n",
    "        # slap a linear layer on the output, taking in all the hidden states\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.word_emb(x)\n",
    "        \n",
    "        # RNN layer outputs a tuple, the output and the final hidden state\n",
    "        # taking the final hidden state as output\n",
    "        x = self.rnn(x)[1]\n",
    "    \n",
    "        return torch.squeeze(x)\n",
    "\n",
    "x, y = next(iter(dl))\n",
    "rnn_model = RNN(dict_length, 100)\n",
    "rnn_model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883e871-61d8-4883-987c-7f36f98ee5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does way better\n",
    "# hidden state updates each time it sees a new word\n",
    "# intuition: probably gets excited when it sees a word like bad/good and ignores the rest\n",
    "lossFun = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print('Epoch: ', epoch)\n",
    "    \n",
    "    loss = one_pass(model, dl, optimizer, lossFun)\n",
    "    print('Loss: ', loss)\n",
    "    \n",
    "    acc = one_pass_acc(model, dl, len(ds))\n",
    "    print('Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08415a2a-18ab-4cb2-bd4a-2da97ad9aa73",
   "metadata": {},
   "source": [
    "## Tools for Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a60bae-91dc-4062-9710-f03e18c89acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim is a great package for word embeddings\n",
    "# easy to train your own!\n",
    "import gensim.downloader\n",
    "\n",
    "# twitter embedding might be helpful for doing NLP related to social media!\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699102d-2d7f-409a-a3c3-2b27144b6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the glove-wiki-gigaword-100\n",
    "# you can freeze the embedding for a model, finetune the embedding, or use it as a starting point for an embedding layer\n",
    "glove_emb = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe240be6-7cec-449d-9f82-fa6c6071ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can easily perform all the fancy features of word embeddings\n",
    "glove_emb.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0527c-e0de-4898-85a6-782a304f00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normed cat vector\n",
    "cat_vec = glove_emb.get_vector('cat')\n",
    "cat_vec / np.linalg.norm(cat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e696c59-43e8-46ee-a570-7c46a7eeb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weights\n",
    "weights = glove_emb.get_normed_vectors()\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd11e0ba-e00b-426d-892f-500df86c9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch makes it easy to load the weights\n",
    "glove_emb_layer = nn.Embedding.from_pretrained(torch.tensor(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c862f-1bd5-44e6-83d6-6c78a54ab8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idx = glove_emb.get_index('cat')\n",
    "glove_emb_layer(torch.tensor(cat_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad35fba-d354-419b-9be1-2a38a76bc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you turn off the gradients when training!\n",
    "for param in glove_emb_layer.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93624f77-ea54-4f87-98a1-8203624020ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_emb_layer = nn.Embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
    "for param in glove_emb_layer.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a5922-8576-4a82-9eb3-8b52bebfd473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# different options for how to perform word2vec training\n",
    "# check out documentation for more options related to sampling frequent vs. infrequent words\n",
    "w2v_model = Word2Vec(# only consider words that show up at least a 100 times\n",
    "                     min_count = 100, \n",
    "                     \n",
    "                     # context window\n",
    "                     window = 2,\n",
    "                     \n",
    "                     #size of embedding\n",
    "                     vector_size = 300)\n",
    "# has methods build_vocab and train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
