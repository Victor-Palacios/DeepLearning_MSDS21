{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e032868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# data augmentation\n",
    "import albumentations as A\n",
    "\n",
    "# pretrained models\n",
    "import torchvision\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb290b-54b8-4d2f-af18-fff71dcd33ab",
   "metadata": {},
   "source": [
    "Image Data from [here](https://www.kaggle.com/andrewmvd/animal-faces)\n",
    "- Animal Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369bebf4-9016-426b-b48c-3228c4e7b1e5",
   "metadata": {},
   "source": [
    "## Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda1aedf-dbfe-493f-a859-659fcadcd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for our image data\n",
    "data_path = 'course_data/afhq'\n",
    "\n",
    "rows = []\n",
    "for dataset in os.listdir(data_path):\n",
    "    for label in os.listdir(data_path + f'/{dataset}'):\n",
    "        for image in os.listdir(data_path + f'/{dataset}' + f'/{label}'):\n",
    "            row = dict()\n",
    "            row['image_file'] = image\n",
    "            row['label'] = label\n",
    "            row['dataset'] = dataset\n",
    "        \n",
    "            # a bit redudant, could build from other data in __getitem__ if wanted\n",
    "            row['image_path'] = data_path + f'/{dataset}' + f'/{label}'\n",
    "            rows.append(row)\n",
    "        \n",
    "df = pd.DataFrame(rows)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120426b8-4e1d-4dea-bcf3-b0f4eb4ba9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation data\n",
    "df_train = df[df['dataset'] == 'train'].reset_index(drop=True)\n",
    "df_val = df[df['dataset'] == 'val'].reset_index(drop=True)\n",
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57230353-94d5-40c1-97a0-799c0ebffa5c",
   "metadata": {},
   "source": [
    "We're going to work with a pre-trained model that takes in images of size 224x224. We will reduce the resolution as a *pre-processings* step rather than on the fly to save time during training.\n",
    "- Notice the time we save during each epoch: 3 seconds for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bef54-d3e6-4628-9fa6-b7cf6d4cfc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(path, size):\n",
    "    img = cv2.imread(path)\n",
    "    \n",
    "    start = time.time()\n",
    "    img = cv2.resize(img, size)\n",
    "    end = time.time()\n",
    "    \n",
    "    cv2.imwrite(path, img)\n",
    "    return end - start\n",
    "\n",
    "# resize all of the images to 256x256\n",
    "total_time_resize = 0.0\n",
    "for idx in tqdm(range(len(df_train))):\n",
    "    row = df_train.iloc[idx]\n",
    "    image_path = row['image_path']\n",
    "    fname = row['image_file']\n",
    "    path = image_path+'/'+fname\n",
    "    \n",
    "    total_time_resize += resize_img(path, (256, 256))\n",
    "    \n",
    "for idx in tqdm(range(len(df_val))):\n",
    "    row = df_train.iloc[idx]\n",
    "    image_path = row['image_path']\n",
    "    fname = row['image_file']\n",
    "    path = image_path+'/'+fname\n",
    "    \n",
    "    total_time_resize += resize_img(path, (256, 256))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb0e02-a3c9-46c4-a7f6-9c49bc78db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee046eb-4c26-4db2-be5f-cc3400e95e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_train.iloc[100]\n",
    "image_path = row['image_path']\n",
    "fname = row['image_file']\n",
    "path = image_path+'/'+fname\n",
    "img = cv2.imread(path)\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff44043-dfca-4ee2-b5a3-ce71956e75e9",
   "metadata": {},
   "source": [
    "## Data Augmentation with [Albumentations](https://github.com/albumentations-team/albumentations)\n",
    "- A suite of very fast transformations for images\n",
    "- Supports masks and keypoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b578b01-e5cd-4b9d-959e-0b881003073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# let's add an augmentation option\n",
    "class AnimalFacesDataset(Dataset):\n",
    "    def __init__(self, df, augment=False):\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        \n",
    "        # label dictionary\n",
    "        self.label_dict = {'cat':0, 'dog':1, 'wild':2}\n",
    "        \n",
    "        # define the transformation\n",
    "        if augment == True:\n",
    "            self.transforms = A.Compose([\n",
    "                # spatial transforms\n",
    "                A.RandomCrop(width=224, height=224),\n",
    "                A.HorizontalFlip(p=.5),\n",
    "                A.VerticalFlip(p=.5),\n",
    "                A.Rotate(limit = 10, \n",
    "                         border_mode = cv2.BORDER_CONSTANT, \n",
    "                         value = 0.0, p = .75),\n",
    "                \n",
    "                # pixel-level transformation\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                \n",
    "                # we will normalize according to ImageNet since we will be using a pre-trained ResNet\n",
    "                # this adjusts from [0,255] to [0,1]\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                \n",
    "                # convert to a tensor and move color channels\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = A.Compose([\n",
    "                # training/valid images have same size\n",
    "                A.CenterCrop(width=224, height=224),\n",
    "                \n",
    "                # normalize\n",
    "                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                \n",
    "                # convert to a tensor and move color channels\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # get ingredients for retrieving image\n",
    "        image_path = row['image_path']\n",
    "        fname = row['image_file']\n",
    "        path = image_path+'/'+fname\n",
    "        \n",
    "        # read the img\n",
    "        img = cv2.imread(path)\n",
    "        \n",
    "        # convert to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # transform the image\n",
    "        # certain transformations expect the uint8 datatype\n",
    "        transformed = self.transforms(image=img.astype(np.uint8))\n",
    "        img = transformed['image']\n",
    "        \n",
    "        label = torch.tensor(self.label_dict[row['label']])\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e490a686-4006-4423-a91d-a4a4ccd04fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = AnimalFacesDataset(df_train, augment=True)\n",
    "dl_train = DataLoader(ds_train, batch_size = 16, shuffle=True)\n",
    "\n",
    "ds_val = AnimalFacesDataset(df_val)\n",
    "dl_val = DataLoader(ds_val, batch_size = 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52261356-1c91-485c-ab2c-072e3dc33e04",
   "metadata": {},
   "source": [
    "Below we double check that this is working properly, and can see the transformation in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab0d6e-71da-45f2-9f64-a4c0c49a5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(ds_train))\n",
    "print(img.shape)\n",
    "\n",
    "# restructure for plt\n",
    "img = np.transpose(np.array(img), (1,2,0))\n",
    "\n",
    "# reverse the normalization\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "for i in range(3):\n",
    "    img[:,:,i] = (img[:,:,i] * std[i]) + mean[i]\n",
    "\n",
    "plt.imshow(img)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb861e-0ace-4bb9-8f99-e1202078f49d",
   "metadata": {},
   "source": [
    "## Pretrained Models\n",
    "- Freezing Layers (feature extraction)\n",
    "- Finetuning (weight initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83406fad-22c7-4bd4-ab35-dc35d150f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drum roll...the pretrained resnet!\n",
    "resnet = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5923144-ced7-4e12-a698-faa5e86de73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the architecture\n",
    "# note how many of the layers are organized in \"BasicBlock\"\n",
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1fbd33-4d5e-47f7-9691-c8b2499e0146",
   "metadata": {},
   "source": [
    "- Notice how the image eventually becomes a 1D vector of dimension 512\n",
    "- In some sense the network has transformed an image into a vector of features helpful for image classification\n",
    "- The last layer is a simple function (linear followed by softmax) on this feature space that predicts an images class\n",
    "- One strategy is to train a new simple function on this **same** feature space for our classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72d09a-78ea-4ec7-aceb-67bfaa081858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(resnet, input_size = (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348cac1-5a90-4795-8c71-7f309c3abb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off gradients for all the parameters\n",
    "for param in resnet.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3239a40b-79e2-44c6-9d3f-3f224e85e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-intialize the last layer for our task\n",
    "print(resnet.fc)\n",
    "resnet.fc = nn.Linear(512, 3)\n",
    "print(resnet.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d64c4b-6851-4b7e-8bc4-36e991409662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initializing the layer reset to default settings\n",
    "for param in resnet.fc.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f819a11-860d-41f2-8598-960119767120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double-check all the parameters\n",
    "for name, param in resnet.named_parameters():\n",
    "    print(f\"{name} gradient is set to\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de65d46-4ac3-4d01-baad-6659d1f09fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the appropriate parameters to the optimizer\n",
    "params_to_update = []\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "optimizer = optim.Adam(params_to_update, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48bbab6-2250-485e-950a-a049857d675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make sure that this actually freezes/trains the layers, take a sample weight\n",
    "print(resnet.conv1.weight[0])\n",
    "print(resnet.fc.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e63ff-6da8-41f5-90d6-b24d640b374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_pass(model, dataloader, optimizer, lossFun, backwards=True, print_loss=False):\n",
    "    \n",
    "    if backwards == True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = lossFun(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if backwards == True:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if print_loss == True:\n",
    "        print(avg_loss)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def one_pass_acc(model, dataloader, num_points):\n",
    "    model.eval()\n",
    "    total_incorrect = 0\n",
    "    \n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        y_pred = softmax(model(x))\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        \n",
    "        total_incorrect += torch.count_nonzero(y - y_pred).item()\n",
    "        \n",
    "    percent_wrong = total_incorrect / num_points\n",
    "    return 1 - percent_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ce83b-c4f2-4120-b15a-f729a6eef0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "lossFun = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print('Epoch: ', epoch)\n",
    "    \n",
    "    train_loss = one_pass(resnet, dl_train, optimizer, lossFun)\n",
    "    train_losses.append(train_loss)\n",
    "    print('Train loss: ', train_loss)\n",
    "    \n",
    "    valid_loss = one_pass(resnet, dl_val, optimizer, lossFun, backwards=False)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print('Valid loss: ', valid_loss)\n",
    "    \n",
    "    train_acc = one_pass_acc(resnet, dl_train, len(ds_train))\n",
    "    valid_acc = one_pass_acc(resnet, dl_val, len(ds_val))\n",
    "    print('Train Acc: ', train_acc)\n",
    "    print('Valid Acc: ', valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf246f-c486-4971-b40f-5c108dc04f15",
   "metadata": {},
   "source": [
    "Note how long it takes to train for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0a828-4c2b-4509-b085-17e1f19f54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet.conv1.weight[0])\n",
    "print(resnet.fc.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb6e8e-d4c2-49c7-9708-7290fb27d28b",
   "metadata": {},
   "source": [
    "If we want to finetune, we can either\n",
    "- use the resnet as a starting point and train by treating the pretrained weights as good weight initilaization OR\n",
    "- we can train different layers at different learning rates (the later the layer, the more we want to adjust the feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3144f5-38d6-4994-b801-f64beaee572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can pass the optimizer groups of parameters rather than all the parameters in one group\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(param_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9f39a-ea40-4fa6-ae0e-7ca5d9e1c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in resnet.named_children():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155115e-eabf-4f10-a5ae-daf8bec10438",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 0.01\n",
    "params = []\n",
    "for i, layer in enumerate(resnet.children()):\n",
    "    if i < 6:\n",
    "        params.append({'params': layer.parameters(), 'lr': max_lr / 100})\n",
    "    elif 5 < i < 9:\n",
    "        params.append({'params': layer.parameters(), 'lr': max_lr / 10})\n",
    "    else:\n",
    "        params.append({'params': layer.parameters()})\n",
    "        \n",
    "# only the parameters we didn't manually set the learning rate for inherit the learning rate set when defining the optimizer\n",
    "optimizer = optim.Adam(params, lr = max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ca47f-cc50-467b-9bab-ce25a26f5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the parameters groups here\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221cce29-a6be-4d9b-97fb-c2a6292b40fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this a bit cleaner you can make a new model class\n",
    "# use model.features1, model.features2, and model.classifier to set the learning rates\n",
    "class Tune_ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tune_ResNet, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        layers = list(resnet.children())[:9]\n",
    "        self.features1 = nn.Sequential(*layers[:6])\n",
    "        self.features2 = nn.Sequential(*layers[6:])\n",
    "        self.classifier = nn.Linear(512, 3)\n",
    "        self.unroll = nn.Flatten()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features1(x)\n",
    "        x = self.features2(x)\n",
    "        x = self.unroll(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model = Tune_ResNet()\n",
    "summary(model, input_size = (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f02122-57fa-452e-8f1a-94c5529d2844",
   "metadata": {},
   "source": [
    "## Training on a GPU\n",
    "- We saw how slow it was to train images on a cpu\n",
    "- PyTorch makes it easy to do this training on a GPU!\n",
    "- Always follow GPU etiquette and check who is running what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adba611-dcdc-4982-bfa1-f4b512e00a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is a GPU available?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265d230-a907-4071-80e0-4c72a6360948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check who is using what\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16ded2-6486-491f-9f06-5205665e1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many devices are there?\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d5cdc-9953-4336-92f7-761e5347ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_no = 0\n",
    "if torch.cuda.is_available() == True:\n",
    "    device = torch.device(device_no)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eaf798-c904-45ab-8879-141e9931687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model parameters to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071d8b1-a383-41f1-bbf7-af78a0647577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's adapt our earlier function\n",
    "def one_pass(model, dataloader, optimizer, lossFun, device, backwards=True, print_loss=False):\n",
    "    \n",
    "    if backwards == True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        \n",
    "        # send labelled data to the device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = lossFun(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if backwards == True:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if print_loss == True:\n",
    "        print(avg_loss)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986f108-ae8c-4eb8-b699-5526ef094d73",
   "metadata": {},
   "source": [
    "Note that\n",
    "- The model can only take inputs on the same device\n",
    "- The output is also on the specified device and cannot interact with tensors on a different device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa035e-d834-47db-a435-bfa2214c2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dl_train))\n",
    "\n",
    "# move to device\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "# perform computation\n",
    "y_pred = model(x)\n",
    "\n",
    "# now its on the cpu again\n",
    "y_pred.cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
