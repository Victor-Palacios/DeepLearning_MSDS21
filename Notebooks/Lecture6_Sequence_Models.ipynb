{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b376ace-3837-418d-9f01-d7def6be3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefa422-45ca-4994-b6ca-16fd9507420a",
   "metadata": {},
   "source": [
    "## The ```nn.RNN``` module\n",
    "Some basic options for ```nn.RNN```\n",
    "- ```input_size```: refers to size of embedding/feature vectors (i.e. number of channels)\n",
    "- ```hidden_size```: desired dimensions of hidden state vector\n",
    "- ```num_layers```: number of RNNs stacked on top\n",
    "- ```batch_first```: If True, the input/output dimension is *(batch size, sequence length, embedding/feature vector size)*, otherwise it is *(sequence length, batch size, embedding/feature vector size)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a48bde-5580-435e-bf9f-3cdd1e1a032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we have a sequence of 300 dimensional vectors\n",
    "# hidden state dimension will be 100\n",
    "basic_rnn = nn.RNN(input_size=300, hidden_size=100, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4a84c-7af7-42a9-80c1-445329be6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's in here?\n",
    "for name, param in basic_rnn.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e044fc-792d-4344-a57c-69807fea18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume we have batch_size=3 and a length 10 sequence of 300 dimensional vectors\n",
    "input_seq = torch.rand((3, 10, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd1ebd-2c87-4f75-afc6-2e1097ba5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get two outputs when we pass a batch to the RNN\n",
    "output = basic_rnn(input_seq)\n",
    "for element in output:\n",
    "    print(element.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad583a-2bc4-40c5-afd6-7c2259c57410",
   "metadata": {},
   "source": [
    "- The first output is a length ten sequence of 100 dimensional vectors (per datapoint in batch of size 3)\n",
    "- These are all the hidden states as we passed the sequence through the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f7e89-cf07-41c3-9816-d99e5c69d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475bfb24-44e2-4b4c-9034-5c2d1e0b7c6b",
   "metadata": {},
   "source": [
    "- The second output is a single 100 dimensional vector (per datapoint in batch of size 3)\n",
    "- This is the *last* hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6048c-2d16-4578-857d-65ce7445c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[1] - output[0][:,-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c15200-284d-460d-8786-d5d0d4d7ae48",
   "metadata": {},
   "source": [
    "We can give the RNN layer a second input: a initial hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1e08e-b81e-42d5-800e-3705a110439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a different initial hidden state changes the output slightly\n",
    "basic_rnn(input_seq)[1] - basic_rnn(input_seq, torch.rand((1, 3, 100)))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c9d521-de64-44c7-a8ca-64cc2bc82307",
   "metadata": {},
   "source": [
    "- We see two sets of weights if we do more than one layer\n",
    "- Note that the $W_{ih}$ weight of the second layer is 100$\\times$100 since the input vectors for the second layer of the RNN are 100-dimensional vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58236eba-2c31-4270-949e-377b9b606e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layer_basic_rnn = nn.RNN(input_size=300, hidden_size=100, num_layers=2, batch_first=True)\n",
    "for name, param in two_layer_basic_rnn.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955c217-72b6-465e-a414-1245f2e838b2",
   "metadata": {},
   "source": [
    "- The shape of our output changes slightly\n",
    "- The first element are the hidden states of the top/last layer\n",
    "- The second element are the hidden states output by the two layers (let's one use this as input to a new RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a3b0e-9a58-4fee-a3db-ca603938c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = two_layer_basic_rnn(input_seq)\n",
    "for element in output:\n",
    "    print(element.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85587e0b-0d68-4c58-85b4-7873bcd88404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla RNN using nn.RNN\n",
    "class Vanilla_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Vanilla_RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        \n",
    "        # here is our g function from the lecture slides\n",
    "        # linear layer turning the i-th hidden state into the i-th output\n",
    "        self.g = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out, hidden = self.rnn(x)\n",
    "        out = self.g(out)\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "v_rnn = Vanilla_RNN(300, 100, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa67c8-d7e9-4748-9367-bf38f40b1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dimension has changing because we did a linear layer from 100-dim to 50-dim\n",
    "for output in v_rnn(input_seq):\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd26686-5f00-4e29-bb71-ecc3dd7c9f9f",
   "metadata": {},
   "source": [
    "## Fancier RNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3231765-bed1-48d9-bd5f-ea1f1c9e41bc",
   "metadata": {},
   "source": [
    "- `nn.GRU` works almost identically to the `nn.RNN` (more parameters inside the $f$ function)\n",
    "- ``nn.LSTM`` is slightly different in that it also has a cell state. So the second output element is a tupe of *(final hidden state, final cell state)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394ee59-6450-4528-a697-cd4806cfaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_gru = nn.GRU(input_size=300, hidden_size=100, num_layers=1, batch_first=True)\n",
    "for output in basic_gru(input_seq):\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68119347-e598-4566-9b91-9a704dca30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_lstm = nn.LSTM(input_size=300, hidden_size=100, num_layers=1, batch_first=True)\n",
    "for output in basic_lstm(input_seq):\n",
    "    try:\n",
    "        print(output.shape)\n",
    "    except:\n",
    "        name = 'hidden'\n",
    "        for ele in output:\n",
    "            print(f'{name} state size:', ele.shape)\n",
    "            name = 'cell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718ce26-12a5-4514-b800-0bcd019c557a",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "- Idea: Take a text and use the shifted text as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773ccf6-7431-494d-9f8f-abd61a75919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('course_data/IMDB_cleaned.csv')\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3773f-05fa-4fe1-82b7-599c6544bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words, send infrequent to unknown\n",
    "from collections import Counter\n",
    "\n",
    "reviews = [review.split(' ') for review in list(df_clean['cleaned'])]\n",
    "word_freq = dict(Counter([token for review in reviews for token in review]).most_common())\n",
    "print(len(word_freq))\n",
    "min_freq = 50\n",
    "word_dict = {}\n",
    "\n",
    "# sending all the unknowns to 0\n",
    "i = 1\n",
    "for word in word_freq:\n",
    "    if word_freq[word] > min_freq:\n",
    "        word_dict[word] = i\n",
    "        i += 1\n",
    "    else:\n",
    "        word_dict[word] = 0\n",
    "\n",
    "# dictionary length        \n",
    "dict_length = max(word_dict.values()) + 1\n",
    "dict_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68badd-ab98-4b5e-94a9-f01173b409b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out unknown tokens for simplicity\n",
    "df_cleaner = pd.DataFrame(list(df_clean.apply(lambda x:\n",
    "                        {'cleaned': ' '.join([token for token in x['cleaned'].split(' ') if word_dict[token] != 0]),\n",
    "                          'sentiment':x['sentiment']}, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f31ab-c6d2-4275-a6a9-079c213d05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean out reviews that are too short\n",
    "min_length = 12\n",
    "print(len(df_clean))\n",
    "df_cleaner = df_cleaner[df_cleaner.apply(lambda x: len(x['cleaned'].split(' ')) >= min_length, axis=1)].reset_index(drop=True)\n",
    "len(df_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0985d6d9-42c4-4980-9241-e25346bf3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# max length here will be maximum length of the sequence predicted\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, df, word_dict, max_length):\n",
    "        self.df = df\n",
    "        self.word_dict = word_dict\n",
    "        self.sent_dict = {'negative': 0, 'positive': 1}\n",
    "        self.max_len = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        review = row['cleaned'].split(' ')\n",
    "        \n",
    "        \n",
    "        x = torch.zeros(self.max_len-1)\n",
    "        y = torch.zeros(self.max_len-1)\n",
    "        \n",
    "        starting_point = random.randint(0,len(review) - (self.max_len))\n",
    "        \n",
    "        # get reviews as a list of integers\n",
    "        for idx in range(self.max_len-1):\n",
    "            x[idx] = self.word_dict[review[starting_point + idx]]\n",
    "            y[idx] = self.word_dict[review[starting_point + idx + 1]]\n",
    "            \n",
    "        \n",
    "        # embedding likes long tensors\n",
    "        return x.long(), y.long()\n",
    "ds = IMDBDataset(df_cleaner, word_dict, 10)\n",
    "\n",
    "# target is the input review shifted over one\n",
    "# i.e. predict next word from first part of the sequence\n",
    "next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72000546-06ee-4b37-a96c-c42a00919a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size = 1000, shuffle=True)\n",
    "for element in tqdm(dl):\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbd5ac-5b0d-4c27-8623-f6e1a15dea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model to generate a synthetic review\n",
    "class LSTM_Gen(nn.Module):\n",
    "    def __init__(self, word_dict, embedding_size, hidden_size):\n",
    "        super(LSTM_Gen, self).__init__()\n",
    "        self.word_dict = word_dict\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # integer to word dictionary\n",
    "        self.idx2word = dict([(x, y) for x, y in zip(self.word_dict.values(), self.word_dict.keys())])\n",
    "        self.idx2word[0] = 'UNK'\n",
    "        \n",
    "        # length of dictionary\n",
    "        dict_length = max(word_dict.values()) + 1\n",
    "        \n",
    "        # embed the words\n",
    "        self.emb = nn.Embedding(dict_length, embedding_size)\n",
    "        \n",
    "        # pass through an LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size)\n",
    "        \n",
    "        # send output through a linear layer\n",
    "        self.linear = nn.Linear(hidden_size, dict_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        out, hidden = self.lstm(x)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out.permute((0, 2, 1))\n",
    "    \n",
    "    # method to generate sequence using LSTM module\n",
    "    def gen_seq(self, start_token, seq_length):\n",
    "        print(start_token)\n",
    "        softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        # embedding of start token\n",
    "        next_emb = self.emb(torch.tensor([[self.word_dict[start_token]]]))\n",
    "        \n",
    "        # initial hidden/cell states\n",
    "        next_state = (torch.zeros((1,1,self.hidden_size)), torch.zeros((1,1,self.hidden_size)))\n",
    "        \n",
    "        # generate a sequence!\n",
    "        for i in range(seq_length):\n",
    "            # use the hidden/cell states for input into next pass through LSTM layer\n",
    "            out, next_state = self.lstm(next_emb, next_state)\n",
    "            \n",
    "            # make prediction\n",
    "            y_pred = self.linear(out)\n",
    "            next_idx = torch.argmax(softmax(y_pred), dim=2)\n",
    "            print(self.idx2word[torch.squeeze(next_idx).item()])\n",
    "            \n",
    "            # embed prediction for input into next pass\n",
    "            next_emb = self.emb(next_idx)\n",
    "            \n",
    "\n",
    "lstm_model = LSTM_Gen(word_dict, embedding_size=100, hidden_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd675d8-e08e-414b-b608-bb6e3a3d5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.gen_seq('first', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f49f96-c10f-41b1-9874-cb3f85fd0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_pass(model, dataloader, optimizer, lossFun, backwards=True, print_loss=False):\n",
    "    \n",
    "    if backwards == True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = lossFun(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if backwards == True:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if print_loss == True:\n",
    "        print(avg_loss)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def one_pass_acc(model, dataloader, num_points):\n",
    "    model.eval()\n",
    "    total_incorrect = 0\n",
    "        \n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        y_pred = torch.argmax(softmax(model(x)), dim=1)\n",
    "        total_incorrect += torch.count_nonzero(y - y_pred).item()\n",
    "        \n",
    "    percent_wrong = total_incorrect / num_points\n",
    "    return 1 - percent_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a5e71-2bdf-4d32-bdc8-8097e1cab9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(list(word_freq.values()))\n",
    "\n",
    "# need to weight the cross entropy loss because of imbalanced dataset\n",
    "weights = [0]\n",
    "for value in word_freq.values():\n",
    "    weights.append(total / (dict_length * value))\n",
    "\n",
    "nn.CrossEntropyLoss(weight=torch.tensor(weights))\n",
    "\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63aba79-7ee8-4aa7-81c3-8d4de86b6876",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print('Epoch: ', epoch)\n",
    "    \n",
    "    loss = one_pass(lstm_model, dl, optimizer, lossFun)\n",
    "    print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23affae-678d-42b4-bc45-a2ffd3b3b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.gen_seq('film', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177511c-553f-47cb-9547-9f188f659d4b",
   "metadata": {},
   "source": [
    "## Seq2Seq\n",
    "- Great introduction [here](https://github.com/bentrevett/pytorch-seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c947aa-ef5b-41a5-9b4c-a765f0bf15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30,000 english-german sentences\n",
    "from torchtext.datasets import Multi30k\n",
    "train_data, valid_data, test_data = Multi30k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b3bc9-4fc7-46d3-90f0-05c0af0b11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246aeef-50d5-4bc9-9f30-c53935cbf018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need tokenizers for english and german\n",
    "import spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download de_core_news_sm\n",
    "\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b81f94-1ecc-48e8-b303-4e4915d790e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_to_csv(iterator, path):\n",
    "    rows = []\n",
    "    for sent_de, sent_en in tqdm(iterator):\n",
    "        # create a state of sentence token\n",
    "        tokenized_text_de = ['<sos>']\n",
    "        tokenized_text_en = ['<sos>']\n",
    "        \n",
    "        # no lemmatization for translation!\n",
    "        for token in spacy_de(sent_de):\n",
    "            if token.text not in ['.', '\\n']:\n",
    "                tokenized_text_de.append(token.text.lower())\n",
    "        for token in spacy_en(sent_en):\n",
    "            if token.text not in ['.', '\\n']:\n",
    "                tokenized_text_en.append(token.text.lower())\n",
    "        tokenized_text_de.append('<eos>')\n",
    "        tokenized_text_en.append('<eos>')\n",
    "        row = {'english': tokenized_text_en,\n",
    "               'german': tokenized_text_de}\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(path)\n",
    "    return df\n",
    "    \n",
    "df_train = clean_to_csv(train_data, 'course_data/Multi30k_train.csv')\n",
    "df_val = clean_to_csv(valid_data, 'course_data/Multi30k_val.csv')\n",
    "df_test = clean_to_csv(test_data, 'course_data/Multi30k_test.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7ea64-cf1e-4d9c-9146-0be83ac27dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('course_data/Multi30k_train.csv').drop(columns=[\"Unnamed: 0\"]).applymap(lambda x: x.strip('][').split(', '))\n",
    "df_val = pd.read_csv('course_data/Multi30k_val.csv').drop(columns=[\"Unnamed: 0\"]).applymap(lambda x: x.strip('][').split(', '))\n",
    "df_test = pd.read_csv('course_data/Multi30k_test.csv').drop(columns=[\"Unnamed: 0\"]).applymap(lambda x: x.strip('][').split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e63e9-5373-48f3-b29d-bef115ae1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab from JUST training data (prevent data leakage)\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(df, col_name, min_freq):\n",
    "    all_words = [token for sentence in list(df[col_name]) for token in sentence if token != '\\n']\n",
    "    \n",
    "    word_freq = dict(Counter(all_words).most_common())\n",
    "    word_dict = {'<unk>' : 0}\n",
    "    \n",
    "    i = 0\n",
    "    for word in word_freq:\n",
    "        if word_freq[word] >= min_freq:\n",
    "            word_dict[word] = i+1\n",
    "            i += 1\n",
    "        else:\n",
    "            word_dict[word] = 0\n",
    "    \n",
    "    idx2word = dict([(x, y) for x, y in zip(word_dict.values(), word_dict.keys())])\n",
    "    idx2word[0] = '<unk>'\n",
    "    \n",
    "    return word_freq, word_dict, idx2word\n",
    "\n",
    "word_freq_en, word_dict_en, idx2word_en = build_vocab(df_train, 'english', 2)\n",
    "word_freq_de, word_dict_de, idx2word_de = build_vocab(df_train, 'german', 2)\n",
    "\n",
    "len(idx2word_en), len(idx2word_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2372aaf7-5d6c-4849-925b-a486eb34a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for idx in range(len(df_train)):\n",
    "    row = df_train.iloc[idx]\n",
    "    if len(row['english']) > max_len:\n",
    "        max_len = len(row['english'])\n",
    "    if len(row['german']) > max_len:\n",
    "        max_len = len(row['german'])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25288b46-914b-4220-bea0-abb1fcb3df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class en2deDataset(Dataset):\n",
    "    def __init__(self, df, word_dict_en, word_dict_de, max_len):\n",
    "        self.df = df\n",
    "        self.word_dict_en = word_dict_en\n",
    "        self.word_dict_de = word_dict_de\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sent_en = row['english']\n",
    "        sent_de = row['german']\n",
    "        \n",
    "        x = torch.zeros(self.max_len)\n",
    "        y = torch.zeros(self.max_len)\n",
    "        \n",
    "        # front pad english sentence\n",
    "        for idx in range(len(sent_en)):\n",
    "            # unknown words get sent to 0\n",
    "            try:\n",
    "                word_idx = self.word_dict_en[sent_en[idx]]\n",
    "            except:\n",
    "                word_idx = 0\n",
    "            x[self.max_len - len(sent_en) + idx] = word_idx\n",
    "        \n",
    "        # back pad german sentence\n",
    "        for idx in range(len(sent_de)):\n",
    "            # unknown words get sent to 0\n",
    "            try:\n",
    "                word_idx = self.word_dict_de[sent_de[idx]]\n",
    "            except:\n",
    "                word_idx = 0\n",
    "            y[idx] = word_idx\n",
    "        \n",
    "        # embedding likes long tensors\n",
    "        return x.long(), y.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d63c1-52aa-4599-a09a-f131e9d38708",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = en2deDataset(df_train, word_dict_en, word_dict_de, max_len)\n",
    "next(iter(ds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5720819-5ec4-4f14-9079-af54399c130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = en2deDataset(df_train, word_dict_en, word_dict_de, max_len)\n",
    "dl_train = DataLoader(ds_train, batch_size=100, shuffle=True)\n",
    "\n",
    "ds_val = en2deDataset(df_val, word_dict_en, word_dict_de, max_len)\n",
    "dl_val = DataLoader(ds_val, batch_size=100, shuffle=False)\n",
    "\n",
    "ds_test = en2deDataset(df_test, word_dict_en, word_dict_de, max_len)\n",
    "dl_test = DataLoader(ds_test, batch_size=100, shuffle=False)\n",
    "    \n",
    "next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982b8fb-92e3-4032-909a-ab070b2e8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we define a simple Encoder with an LSTM\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dict_length_en, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.emb_en = nn.Embedding(dict_length_en, emb_size)\n",
    "        self.rnn = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, batch_first=True)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # don't need the outputs, just the hidden/cell states for input into the decoder\n",
    "        outputs, (hidden, cell) = self.rnn(self.emb_en(x))\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df86d1-b88f-437f-bbda-71e0c204a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the decoder, we need the states from the encoder as input as well as the target sentence\n",
    "# the forward pass represents the prediction of a single German word (the next word in the sentence)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dict_length_de, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.emb_de = nn.Embedding(dict_length_de, emb_size)\n",
    "        self.rnn = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        # output function\n",
    "        self.linear = nn.Linear(hidden_size, dict_length_de)\n",
    "                \n",
    "    def forward(self, input_word, hidden, cell):\n",
    "        \n",
    "        input_emb = self.emb_de(input_word)\n",
    "        \n",
    "        # output the next hidden/cell states\n",
    "        output, (hidden, cell) = self.rnn(input_emb, (hidden, cell))\n",
    "        \n",
    "        # prediction for next word\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f9dd6-971c-4f7c-87ae-c3d03bd8a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, dict_length_en, dict_length_de, emb_size, hidden_size, max_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(dict_length_en, emb_size, hidden_size)\n",
    "        self.decoder = Decoder(dict_length_de, emb_size, hidden_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.max_len = max_len\n",
    "        self.output_size = dict_length_de\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        hidden, cell = self.encoder(x)\n",
    "    \n",
    "        next_word = y[:, 0:1]\n",
    "        prediction = torch.zeros((y.shape[0], self.output_size, y.shape[1]))\n",
    "        \n",
    "        # first token is always <sos>\n",
    "        prediction[:, 1, 0] = 1\n",
    "        \n",
    "        for i in range(self.max_len-1):\n",
    "            \n",
    "            output, (hidden, cell) = self.decoder(next_word, hidden, cell)\n",
    "            prediction[:, :, i+1] = torch.squeeze(output)\n",
    "            \n",
    "            # can implement teacher forcing here (sometimes use target word rather than predicted word for next token)\n",
    "            teacher_forcing_prob = random.uniform(0, 1)\n",
    "            #teacher_forcing_prob = 1.0\n",
    "            if teacher_forcing_prob > 0.5:\n",
    "                next_word = torch.argmax(self.softmax(output), dim=2)\n",
    "            else:\n",
    "                next_word = y[:, (i+1):(i+2)]\n",
    "            \n",
    "                        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317aa05-2b18-4a04-b2b9-2f485da6e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(len(idx2word_en), len(idx2word_de), 100, 100, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d7f7b-1556-47af-b58d-6495de8558ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dl_train))\n",
    "model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7ec45-7b31-4a30-9b48-629f416a3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_pass(model, dataloader, optimizer, lossFun, backwards=True, print_loss=False):\n",
    "    \n",
    "    if backwards == True:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        \n",
    "        y_pred = model(x, y)\n",
    "        loss = lossFun(y_pred, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if backwards == True:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    if print_loss == True:\n",
    "        print(avg_loss)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def one_pass_acc(model, dataloader, num_points):\n",
    "    model.eval()\n",
    "    total_incorrect = 0\n",
    "        \n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        y_pred = torch.argmax(softmax(model(x, y)), dim=1)\n",
    "        total_incorrect += torch.count_nonzero(y - y_pred).item()\n",
    "        \n",
    "    percent_wrong = total_incorrect / num_points\n",
    "    return 1 - percent_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83924ef-41cf-4216-9f9a-ad0eebb5a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c39de-4557-4835-a005-d3f75c04a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print('Epoch: ', epoch)\n",
    "    \n",
    "    loss_train = one_pass(model, dl_train, optimizer, lossFun)\n",
    "    print('Loss: ', loss_train)\n",
    "    \n",
    "    #acc_train = one_pass_acc(model, dl_train, len(ds_train))\n",
    "    #print('Accuracy: ', acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d9ef2-7040-4862-9a16-2a35919e65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a translation\n",
    "softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "# one batch\n",
    "x, y = next(iter(dl_train))\n",
    "y_pred = model(x, y)\n",
    "# english sentence\n",
    "sent_en = []\n",
    "for index in x[0]:\n",
    "    next_word = idx2word_en[index.item()].strip(\"''\")\n",
    "    if next_word not in ['<sos>', '<eos>', '<unk>']:\n",
    "        sent_en.append(next_word)\n",
    "print(' '.join(sent_en))\n",
    "\n",
    "sent_de = []\n",
    "for index in torch.argmax(model.softmax(y_pred), dim=1)[0]:\n",
    "    next_word = idx2word_de[index.item()].strip(\"''\")\n",
    "    if next_word not in ['<sos>', '<eos>', '<unk>']:\n",
    "        sent_de.append(next_word)\n",
    "print(' '.join(sent_de))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d30e3-c82f-4b39-8bbb-95eaa43a4d43",
   "metadata": {},
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b95dc-48a8-4e67-b679-152da323bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class some_loss(nn.Module):\n",
    "    def __init__(self, hyperparam):\n",
    "        super(some_loss, self).__init__()\n",
    "        self.hyperparam = hyperparam\n",
    "        \n",
    "    \n",
    "    def forward(self, y_pred, y):\n",
    "        diff = y_pred - y\n",
    "        \n",
    "        # average over each entry and batch size\n",
    "        torch.norm(diff) / torch.numel(doff)\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
